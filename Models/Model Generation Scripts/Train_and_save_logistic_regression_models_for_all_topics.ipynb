{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instructions\n",
        "\n",
        "This script is what we used to create our Logistic Regression models for the top 5 most common topics. It can be run out-of-the-box, and the models will be saved in the current directory, namely `Model Generation Scripts`\n",
        "\n",
        "Note that the code is somewhat agnostic to model type, and it is trivial to substitute a different sklearn model, e.g. RandomForest, KNN, etc."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D2Pybzkkk9Cd"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Hebrew stopwords set we used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gewBHigXn9Bb"
      },
      "outputs": [],
      "source": [
        "HEBREW_STOPWORDS_1 = ['אני',\n",
        "'את',\n",
        "'אתה',\n",
        "'אנחנו',\n",
        "'אתן',\n",
        "'אתם',\n",
        "'הם',\n",
        "'הן',\n",
        "'היא',\n",
        "'הוא',\n",
        "'שלי',\n",
        "'שלו',\n",
        "'שלך',\n",
        "'שלה',\n",
        "'שלנו',\n",
        "'שלכם',\n",
        "'שלכן',\n",
        "'שלהם',\n",
        "'שלהן',\n",
        "'לי',\n",
        "'לו',\n",
        "'לה',\n",
        "'לנו',\n",
        "'לכם',\n",
        "'לכן',\n",
        "'להם',\n",
        "'להן',\n",
        "'אותה',\n",
        "'אותו',\n",
        "'זה',\n",
        "'זאת',\n",
        "'אלה',\n",
        "'אלו',\n",
        "'תחת',\n",
        "'מתחת',\n",
        "'מעל',\n",
        "'בין',\n",
        "'עם',\n",
        "'עד',\n",
        "'נגר',\n",
        "'על',\n",
        "'אל',\n",
        "'מול',\n",
        "'של',\n",
        "'אצל',\n",
        "'כמו',\n",
        "'אחר',\n",
        "'אותו',\n",
        "'בלי',\n",
        "'לפני',\n",
        "'אחרי',\n",
        "'מאחורי',\n",
        "'עלי',\n",
        "'עליו',\n",
        "'עליה',\n",
        "'עליך',\n",
        "'עלינו',\n",
        "'עליכם',\n",
        "'לעיכן',\n",
        "'עליהם',\n",
        "'עליהן',\n",
        "'כל',\n",
        "'כולם',\n",
        "'כולן',\n",
        "'כך',\n",
        "'ככה',\n",
        "'כזה',\n",
        "'זה',\n",
        "'זות',\n",
        "'אותי',\n",
        "'אותה',\n",
        "'אותם',\n",
        "'אותך',\n",
        "'אותו',\n",
        "'אותן',\n",
        "'אותנו',\n",
        "'ואת',\n",
        "'את',\n",
        "'אתכם',\n",
        "'אתכן',\n",
        "'איתי',\n",
        "'איתו',\n",
        "'איתך',\n",
        "'איתה',\n",
        "'איתם',\n",
        "'איתן',\n",
        "'איתנו',\n",
        "'איתכם',\n",
        "'איתכן',\n",
        "'יהיה',\n",
        "'תהיה',\n",
        "'היתי',\n",
        "'היתה',\n",
        "'היה',\n",
        "'להיות',\n",
        "'עצמי',\n",
        "'עצמו',\n",
        "'עצמה',\n",
        "'עצמם',\n",
        "'עצמן',\n",
        "'עצמנו',\n",
        "'עצמהם',\n",
        "'עצמהן',\n",
        "'מי',\n",
        "'מה',\n",
        "'איפה',\n",
        "'היכן',\n",
        "'במקום שבו',\n",
        "'אם',\n",
        "'לאן',\n",
        "'למקום שבו',\n",
        "'מקום בו',\n",
        "'איזה',\n",
        "'מהיכן',\n",
        "'איך',\n",
        "'כיצד',\n",
        "'באיזו מידה',\n",
        "'מתי',\n",
        "'בשעה ש',\n",
        "'כאשר',\n",
        "'כש',\n",
        "'למרות',\n",
        "'לפני',\n",
        "'אחרי',\n",
        "'מאיזו סיבה',\n",
        "'הסיבה שבגללה',\n",
        "'למה',\n",
        "'מדוע',\n",
        "'לאיזו תכלית',\n",
        "'כי',\n",
        "'יש',\n",
        "'אין',\n",
        "'אך',\n",
        "'מנין',\n",
        "'מאין',\n",
        "'מאיפה',\n",
        "'יכל',\n",
        "'יכלה',\n",
        "'יכלו',\n",
        "'יכול',\n",
        "'יכולה',\n",
        "'יכולים',\n",
        "'יכולות',\n",
        "'יוכלו',\n",
        "'יוכל',\n",
        "'מסוגל',\n",
        "'לא',\n",
        "'רק',\n",
        "'אולי',\n",
        "'אין',\n",
        "'לאו',\n",
        "'אי',\n",
        "'כלל',\n",
        "'נגד',\n",
        "'אם',\n",
        "'עם',\n",
        "'אל',\n",
        "'אלה',\n",
        "'אלו',\n",
        "'אף',\n",
        "'על',\n",
        "'מעל',\n",
        "'מתחת',\n",
        "'מצד',\n",
        "'בשביל',\n",
        "'לבין',\n",
        "'באמצע',\n",
        "'בתוך',\n",
        "'דרך',\n",
        "'מבעד',\n",
        "'באמצעות',\n",
        "'למעלה',\n",
        "'למטה',\n",
        "'מחוץ',\n",
        "'מן',\n",
        "'לעבר',\n",
        "'מכאן',\n",
        "'כאן',\n",
        "'הנה',\n",
        "'הרי',\n",
        "'פה',\n",
        "'שם',\n",
        "'אך',\n",
        "'ברם',\n",
        "'שוב',\n",
        "'אבל',\n",
        "'מבלי',\n",
        "'בלי',\n",
        "'מלבד',\n",
        "'רק',\n",
        "'בגלל',\n",
        "'מכיוון',\n",
        "'עד',\n",
        "'אשר',\n",
        "'ואילו',\n",
        "'למרות',\n",
        "'אס',\n",
        "'כמו',\n",
        "'כפי',\n",
        "'אז',\n",
        "'אחרי',\n",
        "'כן',\n",
        "'לכן',\n",
        "'לפיכך',\n",
        "'מאד',\n",
        "'עז',\n",
        "'מעט',\n",
        "'מעטים',\n",
        "'במידה',\n",
        "'שוב',\n",
        "'יותר',\n",
        "'מדי',\n",
        "'גם',\n",
        "'כן',\n",
        "'נו',\n",
        "'אחר',\n",
        "'אחרת',\n",
        "'אחרים',\n",
        "'אחרות',\n",
        "'אשר',\n",
        "'או'\n",
        "]\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Cleaned Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7mku0d2yKywm",
        "outputId": "e013ae8b-e532-4aeb-d405-d46971a30f97"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pm_ref</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>וכלי חרש אשר יגע בו הזב ישבר וכל כלי עץ ישטף ב...</td>\n",
              "      <td>Leviticus 15:13</td>\n",
              "      <td>טבילה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>והיה לפנות ערב ירחץ במים וכבא השמש יבא אל תוך ...</td>\n",
              "      <td>Deuteronomy 23:12</td>\n",
              "      <td>טבילה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>אמר לו רבי יהודה והלא כהנים מבעוד יום הם טובלים</td>\n",
              "      <td>Berakhot 2b:13</td>\n",
              "      <td>טבילה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>והתנן טמא שירד לטבול ספק טבל ספק לא טבל ואפילו...</td>\n",
              "      <td>Eruvin 35b:2</td>\n",
              "      <td>טבילה</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>דאמר רב יהודה אמר שמואל כל המצוות מברך עליהן ע...</td>\n",
              "      <td>Pesachim 7b:9-12</td>\n",
              "      <td>טבילה</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text             pm_ref  topic\n",
              "0  וכלי חרש אשר יגע בו הזב ישבר וכל כלי עץ ישטף ב...    Leviticus 15:13  טבילה\n",
              "1  והיה לפנות ערב ירחץ במים וכבא השמש יבא אל תוך ...  Deuteronomy 23:12  טבילה\n",
              "2   אמר לו רבי יהודה והלא כהנים מבעוד יום הם טובלים      Berakhot 2b:13  טבילה\n",
              "3  והתנן טמא שירד לטבול ספק טבל ספק לא טבל ואפילו...       Eruvin 35b:2  טבילה\n",
              "4  דאמר רב יהודה אמר שמואל כל המצוות מברך עליהן ע...   Pesachim 7b:9-12  טבילה"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CLEANED_DATA_PATH = '../../Data/Cleaned Data/good_df.json'\n",
        "good_df = pd.read_json(CLEANED_DATA_PATH)\n",
        "good_df = good_df[['text','pm_ref', 'topic']]\n",
        "good_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Topic-Sensitive Positive/Negative Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ltX5HLBhtIzI"
      },
      "outputs": [],
      "source": [
        "def create_single_topic_df(fixed_data, topic, random_state=613, loc ='fixed_a_text'):\n",
        "  using = fixed_data.copy()\n",
        "  using['is_target'] = np.where(using['topic']==(topic), 1,0)\n",
        "  positive = using[using['is_target']==1]\n",
        "  using = using[using[loc].isin(positive[loc]) == False]\n",
        "  negative = using.sample(len(positive.index), random_state=random_state)\n",
        "  combined = pd.concat([positive, negative], axis=0)\n",
        "  print(\"%s, number of topics %d\" % (topic, positive['is_target'].sum() ))\n",
        "  return combined"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing (Vectorize Data and Train/Test Split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q2XQmK8AMTFU"
      },
      "outputs": [],
      "source": [
        "def split_and_transform(single_topic_df, vct=CountVectorizer, bag_size=10_000, stopwords=None, random_state=613, loc ='fixed_a_text'):\n",
        "  texts = single_topic_df[loc]\n",
        "  targets = single_topic_df['is_target'].to_numpy()\n",
        "  train_text, test_text, train_label, test_label = train_test_split(texts.to_numpy(), targets, random_state=random_state, test_size=0.2, stratify=targets)\n",
        "  vectorizer = vct(max_features=bag_size, stop_words=stopwords)\n",
        "  \n",
        "  # only fit vectorizer on training data\n",
        "  train_bag = vectorizer.fit_transform(train_text)\n",
        "  test_bag = vectorizer.transform(test_text)\n",
        "  pickle.dump(vectorizer, open(t+\"_bag.pkl\",\"wb\"))\n",
        "  return train_bag, test_bag, train_label, test_label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train an `sklearn` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XVUouqckMVCl"
      },
      "outputs": [],
      "source": [
        "def train_sklearny(clf, train_bag, train_label):\n",
        "  clf.fit(train_bag, train_label)\n",
        "  return clf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate an `sklearn` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1eQRL4WHMWRb"
      },
      "outputs": [],
      "source": [
        "def evaluate_sklearny(clf, test_bag, test_label, verbose=True):\n",
        "  res = ''\n",
        "  predictions = clf.predict(test_bag)\n",
        "  res = res + f'\\nscore: {clf.score(test_bag,test_label)}'\n",
        "  if verbose: res = res + f'\\nClassification Report: \\n{classification_report(test_label, predictions)}'\n",
        "  return res"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Export"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Picking the top 5 most common topics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TddUt7CqXvE",
        "outputId": "ced66c33-fc50-4d8c-c91b-174ca8c040ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['למוד', 'תורה', 'תפלה', 'תשובה', 'ישראל']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topics = good_df.value_counts('topic').keys().tolist()[:5]\n",
        "topics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pre-processing/train/evaluate/save loop for each topic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljvvHUs6prdE",
        "outputId": "1b02b007-0507-4c4f-e60d-0c9901b146d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "למוד, number of topics 998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['באיזו', 'בו', 'במקום', 'בשעה', 'הסיבה', 'לאיזו', 'למקום', 'מאיזו', 'מידה', 'מקום', 'סיבה', 'שבגללה', 'שבו', 'תכלית'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "score: 0.8475\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       200\n",
            "           1       0.86      0.83      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "תורה, number of topics 894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['באיזו', 'בו', 'במקום', 'בשעה', 'הסיבה', 'לאיזו', 'למקום', 'מאיזו', 'מידה', 'מקום', 'סיבה', 'שבגללה', 'שבו', 'תכלית'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "score: 0.8854748603351955\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.97      0.89       179\n",
            "           1       0.97      0.80      0.87       179\n",
            "\n",
            "    accuracy                           0.89       358\n",
            "   macro avg       0.90      0.89      0.88       358\n",
            "weighted avg       0.90      0.89      0.88       358\n",
            "\n",
            "תפלה, number of topics 837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['באיזו', 'בו', 'במקום', 'בשעה', 'הסיבה', 'לאיזו', 'למקום', 'מאיזו', 'מידה', 'מקום', 'סיבה', 'שבגללה', 'שבו', 'תכלית'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "score: 0.8507462686567164\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       168\n",
            "           1       0.89      0.80      0.84       167\n",
            "\n",
            "    accuracy                           0.85       335\n",
            "   macro avg       0.85      0.85      0.85       335\n",
            "weighted avg       0.85      0.85      0.85       335\n",
            "\n",
            "תשובה, number of topics 711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['באיזו', 'בו', 'במקום', 'בשעה', 'הסיבה', 'לאיזו', 'למקום', 'מאיזו', 'מידה', 'מקום', 'סיבה', 'שבגללה', 'שבו', 'תכלית'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "score: 0.8771929824561403\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89       143\n",
            "           1       0.95      0.80      0.87       142\n",
            "\n",
            "    accuracy                           0.88       285\n",
            "   macro avg       0.89      0.88      0.88       285\n",
            "weighted avg       0.89      0.88      0.88       285\n",
            "\n",
            "ישראל, number of topics 695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['באיזו', 'בו', 'במקום', 'בשעה', 'הסיבה', 'לאיזו', 'למקום', 'מאיזו', 'מידה', 'מקום', 'סיבה', 'שבגללה', 'שבו', 'תכלית'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "score: 0.8165467625899281\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.82       139\n",
            "           1       0.85      0.77      0.81       139\n",
            "\n",
            "    accuracy                           0.82       278\n",
            "   macro avg       0.82      0.82      0.82       278\n",
            "weighted avg       0.82      0.82      0.82       278\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for t in topics: \n",
        "  single_topic_df = create_single_topic_df(good_df, t, 613, 'text')\n",
        "  train_bag, test_bag, train_label, test_label = split_and_transform(\n",
        "      single_topic_df, vct=CountVectorizer, bag_size=10_000, stopwords=HEBREW_STOPWORDS_1, loc= 'text')\n",
        "  model = train_sklearny(LogisticRegression(), train_bag, train_label)\n",
        "  eval = evaluate_sklearny(model, test_bag, test_label, verbose=True)\n",
        "  print(eval)\n",
        "  pickle.dump(model, open(t+\".pkl\",\"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMgwIUzIsQms"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
